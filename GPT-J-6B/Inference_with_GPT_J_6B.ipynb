{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference with GPT-J-6B.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluebrain-ai/gpt6jb/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q8XR3P0GsP3"
      },
      "source": [
        "## Inference with GPT-J-6B\n",
        "\n",
        "In this notebook, we are going to perform inference (i.e. generate new text) with EleutherAI's [GPT-J-6B model](https://github.com/kingoflolz/mesh-transformer-jax/), which is a 6 billion parameter GPT model trained on [The Pile](https://arxiv.org/abs/2101.00027), a huge publicly available text dataset, also collected by EleutherAI. The model itself was trained on TPUv3s using JAX and Haiku (the latter being a neural net library on top of JAX).\n",
        "\n",
        "[EleutherAI](https://www.eleuther.ai/) itself is a group of AI researchers doing awesome AI research (and making everything publicly available and free to use). They've also created [GPT-Neo](https://github.com/EleutherAI/gpt-neo), which are smaller GPT variants (with 125 million, 1.3 billion and 2.7 billion parameters respectively). Check out their models on the hub [here](https://huggingface.co/EleutherAI).\n",
        "\n",
        "NOTE: this notebook requires at least 12.1GB of CPU memory. I'm personally using Colab Pro to run it (and set runtime to GPU - high RAM usage). Unfortunately, the free version of Colab only provides 10 GB of RAM, which isn't enough.\n",
        "\n",
        "## Install dependencies\n",
        "\n",
        "We will install Transformers from source for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0l8--PK79bH",
        "outputId": "16dcaf3f-6790-4f31-811f-3b3f70bfa887"
      },
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install accelerate"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.20.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYeldWLtFlvi"
      },
      "source": [
        "## Load model and tokenizer\n",
        "\n",
        "First, we load the model from the hub. We select the \"float16\" revision, which means that all parameters are stored using 16 bits, rather than the default float32 ones (which require twice as much RAM memory). We also set `low_cpu_mem_usage` to `True` (which was introduced in [this PR](https://github.com/huggingface/transformers/pull/13466)), in order to only load the model once into CPU memory.\n",
        "\n",
        "Next, we move the model to the GPU and load the corresponding tokenizer, which we'll use to prepare text for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnORS2zP7-uB"
      },
      "source": [
        "import torch\n",
        "from transformers import GPTJForCausalLM, AutoTokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\")\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vnZfO1HGSS7"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Here, we can provide a custom prompt, prepare that prompt using the tokenizer for the model (the only input required for the model are the `input_ids`). We then move the `input_ids` also to the GPU, and use the `.generate()` method to generate tokens autoregressively. Note that this method supports various decoding methods, including beam search and top k sampling. All details can be found in this [blog post](https://huggingface.co/blog/how-to-generate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "RdOynYcY8jb1",
        "outputId": "0d1fe345-5265-495f-f4f9-3048131327c7"
      },
      "source": [
        "prompt = \"The Belgian national football team \"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "generated_ids = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200)\n",
        "generated_text = tokenizer.decode(generated_ids[0])\n",
        "print(generated_text)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ebfabe1c2952>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The Belgian national football team \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42MAqJ0bl-IX"
      },
      "source": [
        "Note that one of the most interesting properties of large GPT models is that they are capable of so-called \"few-shot learning\". This means that, given only a few examples in a text prompt, the model is capable of quickly generalizing to new, unseen examples. So you can use this model for example to do few-shot text classification, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la40Jv40mR_7",
        "outputId": "b7e9d4d9-91a1-4806-adac-c9442ba3a447"
      },
      "source": [
        "prompt = \"\"\"\n",
        "Sentence: This movie is very nice.\n",
        "Sentiment: positive\n",
        "\n",
        "#####\n",
        "\n",
        "Sentence: I hated this movie, it sucks.\n",
        "Sentiment: negative\n",
        "\n",
        "#####\n",
        "\n",
        "Sentence: This movie was actually pretty funny.\n",
        "Sentiment: positive\n",
        "\n",
        "#####\n",
        "\n",
        "Sentence: This movie could have been better.\n",
        "Sentiment: \"\"\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "generated_ids = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200)\n",
        "generated_text = tokenizer.decode(generated_ids[0])\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence: This movie is very nice.\n",
            "Sentiment: positive\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: I hated this movie, it sucks.\n",
            "Sentiment: negative\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: This movie was actually pretty funny.\n",
            "Sentiment: positive\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: This movie could have been better.\n",
            "Sentiment:  neutral\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: This movie is really fun to watch.\n",
            "Sentiment: positive\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: I would recommend this movie.\n",
            "Sentiment:  neutral\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: This movie was good for a popcorn flick.\n",
            "Sentiment: positive\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: I liked this movie.\n",
            "Sentiment:  neutral\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: I hated this movie, it was terrible.\n",
            "Sentiment:  negative\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: This\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U-vESCAmgjW"
      },
      "source": [
        "Another note: as GPT-J-6B was trained on The Pile (which includes a lot of Github code), the model is capable of performing code generation (similar to OpenAI's Codex model). Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzAb7JdfmnoX",
        "outputId": "91622307-de8a-4d19-a6aa-16bd4fe0f451"
      },
      "source": [
        "prompt = \"\"\"Instruction: Generate a Python function that lets you reverse a list.\n",
        "\n",
        "Answer: \"\"\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "generated_ids = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200)\n",
        "generated_text = tokenizer.decode(generated_ids[0])\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction: Generate a Python function that lets you reverse a list.\n",
            "\n",
            "Answer: \n",
            "import random\n",
            "\n",
            "def reverse_list(aList):\n",
            "        i = len(aList)\n",
            "        x = 0\n",
            "        while x < len(aList):\n",
            "                if aList[x] < aList[0]:\n",
            "                        aList[x] = random.choice(aList[x])\n",
            "                else:\n",
            "                        aList[x] = random.choice(aList[x])\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Vg5pSUkhSk"
      },
      "source": [
        "## Batched inference\n",
        "\n",
        "You can also perform inference on batches of prompts, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvg0ta_QAoQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0788b92d-a0c4-4a10-ff20-1d46ed28380b"
      },
      "source": [
        "# this is a single input batch with size 3\n",
        "texts = [\"Once there was a man \", \"The weather today will be \", \"The best football player in the world is \"]\n",
        "\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(**encoding, do_sample=True, temperature=0.9, max_length=50)\n",
        "generated_texts = tokenizer.batch_decode(\n",
        "    generated_ids, skip_special_tokens=True)\n",
        "\n",
        "for text in generated_texts:\n",
        "  print(\"---------\")\n",
        "  print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------\n",
            "Once there was a man  \n",
            "who had many wonderful talents, and one of them was that he was tall and straight, and so he walked over hill and dale, and wherever he walked he could make the grass grow wherever\n",
            "---------\n",
            "The weather today will be   cool with a chance of snow in the overnight hours.  \n",
            "Mostly cloudy skies will be on tap for the day. Overnight lows will dip to \n",
            "around freezing in the lower elev\n",
            "---------\n",
            "The best football player in the world is ____:\n",
            "\n",
            "A) Lionel Messi\n",
            "\n",
            "B) Cristiano Ronaldo\n",
            "\n",
            "C) Zlatan Ibrahimovic\n",
            "\n",
            "Now let's try making sense of the questions. You should know that \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDF9dvqfpNOl"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}